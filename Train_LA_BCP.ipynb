{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import h5py\n",
    "import os \n",
    "import random \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.measure import label \n",
    "\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params \n",
    "class params: \n",
    "    def __init__(self):\n",
    "        self.root_path = 'LA'\n",
    "        self.exp = 'BCP' \n",
    "        self.model = 'Unet'\n",
    "        self.pre_max_iterations = 200\n",
    "        self.self_max_iteration = 100 \n",
    "        self.max_samples = 80 \n",
    "        self.labeles_bs = 4\n",
    "        self.bacth_size = 8 \n",
    "        self.base_lr = 0.01 \n",
    "        self.deterministic = 1 \n",
    "        self.labelnum = 8 \n",
    "        self.consistency = 1.0 \n",
    "        self.consistency_rampup = 40.0 \n",
    "        self.magnitude = 10.0 \n",
    "        self.seed = 10\n",
    "    \n",
    "        # Setting of BCP \n",
    "        self.u_weight = 0.5 \n",
    "        self.mask_ratio = 2/3 \n",
    "\n",
    "        # Setting of mixup \n",
    "        self.u_alpha = 2.0 \n",
    "        self.loss_weight = 0.5\n",
    "\n",
    "\n",
    "args = params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LAHeart(Dataset):\n",
    "    def __init__(self, base_dir, split='train', transform=None, num=None):\n",
    "        self._base_dir = base_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.sample_list = []\n",
    "        \n",
    "        # Path for train/test list\n",
    "        list_file = os.path.join(self._base_dir, f\"{split}.list\")\n",
    "        if not os.path.isfile(list_file):\n",
    "            raise ValueError(f\"The {split} list file is missing: {list_file}\")\n",
    "        \n",
    "        with open(list_file, 'r') as file:\n",
    "            self.sample_list = [item.strip() for item in file.readlines()]\n",
    "        \n",
    "        if num is not None:\n",
    "            self.sample_list = self.sample_list[:num]\n",
    "\n",
    "        print(f\"Mode = {self.split}, total samples: {len(self.sample_list)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        case = self.sample_list[index]\n",
    "        file_path = os.path.join(self._base_dir, f'2018LA_Seg_Training Set/{case}/mri_norm2.h5')\n",
    "        \n",
    "        # Load data safely\n",
    "        try:\n",
    "            with h5py.File(file_path, 'r') as h5f:\n",
    "                image = h5f['image'][:]\n",
    "                label = h5f['label'][:]\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        sample = {'image': image, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode = train, total samples: 80\n",
      "Image.shape = (203, 142, 88)\n",
      "Label.shape = (203, 142, 88)\n"
     ]
    }
   ],
   "source": [
    "train_db = LAHeart(\n",
    "    base_dir= 'LA', \n",
    "    split= 'train'\n",
    ")\n",
    "\n",
    "sample =train_db.__getitem__(10)\n",
    "print(f'Image.shape = {sample['image'].shape}')\n",
    "print(f'Label.shape = {sample['label'].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rot_flip(image, label): \n",
    "    k = np.random.randint(0, 4, 1) \n",
    "    image = np.rot90(image, k) \n",
    "    label = np.rot90(label, k) \n",
    "\n",
    "    axis = np.random.randint(0, 2)\n",
    "    image = np.flip(image, axis) \n",
    "    label = np.flip(label, axis) \n",
    "\n",
    "    return image, label \n",
    "\n",
    "class RandomRotFlip: \n",
    "    def __call__(self, sample): \n",
    "        image, label = sample['image'], sample['label']\n",
    "        image, label = random_rot_flip(image, label) \n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        return sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop: \n",
    "    def __init__(self, output_size, with_sdf= False): \n",
    "        self.output_size = output_size \n",
    "        self.with_sdf = with_sdf\n",
    "    \n",
    "    def __call__(self, sample): \n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        if self.with_sdf: \n",
    "            sdf = sample['sdf']\n",
    "        \n",
    "        if label.shape[0] <= self.output_size[0] or label.shape[1] <= self.output_size[1] or label.shape[2] <= self.output_size[2]: \n",
    "            pw = max((self.output_size[0] - label.shape[0]) // 2 + 3, 0) \n",
    "            ph = max((self.output_size[1] - label.shape[1]) // 2 + 3, 0)\n",
    "            pd = max((self.output_size[2] - label.shape[2]) // 2 + 3, 0)\n",
    "            image = np.pad(image, [(pw, pw), (ph, ph),(pd, pd)], mode= 'constant', constant_values= 0)\n",
    "            label = np.pad(label, [(pw, pw), (ph, ph), (pd, pd)], mode= 'constant', constant_values= 0) \n",
    "\n",
    "            if self.with_sdf: \n",
    "                sdf = np.pad(sdf, [(pw, pw), (ph, ph), (pd, pd)], mode= 'constant', constant_values= 0) \n",
    "\n",
    "        (w, h,d) = image.shape \n",
    "        w1 = np.random.randint(0, w - self.output_size[0])\n",
    "        h1 = np.random.randint(0, h - self.output_size[1])\n",
    "        d1 = np.random.randint(0, d - self.output_size[2])\n",
    "    \n",
    "        image = image[w1 : w1 + self.output_size[0], h1 : h1 + self.output_size[1], d1 : d1 + self.output_size[2]]\n",
    "        label = label[w1: w1 + self.output_size[0], h1 : h1 + self.output_size[1], d1 : d1 + self.output_size[2]]\n",
    "\n",
    "        if self.with_sdf: \n",
    "            sdf = sdf[w1 : w1 + self.output_size[0], h1 : h1 + self.output_size[1], d1 : d1 + self.output_size[2]]\n",
    "            return {'image': image, 'label': label, 'sdf': sdf}\n",
    "        else: \n",
    "            return {'image': image, 'label': label}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(tensor, nclasses): \n",
    "    \"\"\"\n",
    "    Input (tensor): Nx1xHxW \n",
    "    \"\"\"\n",
    "    assert tensor.max().item() < nclasses\n",
    "    assert tensor.min().item() >= 0 \n",
    "\n",
    "    size = list(tensor.size())\n",
    "    assert size[1] == 1 \n",
    "    size[1] = nclasses\n",
    "    one_hot = torch.zeros(*size) \n",
    "    if tensor.is_cuda: \n",
    "        one_hot = one_hot.cuda(tensor.device) \n",
    "    one_hot = one_hot.scatter_(1, tensor, 1) \n",
    "    return one_hot \n",
    "\n",
    "def get_probability(logits): \n",
    "    \"\"\"\n",
    "    Get the probability from logitis  \n",
    "    \"\"\"\n",
    "    size = logits.size() \n",
    "    if size[1] > 1: \n",
    "        pred = F.softmax(logits, dim= 1) \n",
    "        nclass = size[1] \n",
    "    else: \n",
    "        pred = F.sigmoid(logits) \n",
    "        pred = torch.cat([1 - pred, pred], dim= 1) \n",
    "    \n",
    "    return pred, nclass\n",
    "\n",
    "\n",
    "class mask_DiceLoss(nn.Module): \n",
    "    def __init__(self, nclass, class_weights = None, smooth= 1e-5): \n",
    "        super(mask_DiceLoss, self).__init__() \n",
    "        self.smooth = smooth \n",
    "        if class_weights is None: \n",
    "            self.class_weights = nn.Parameter(torch.ones((1, nclass)).type(torch.float32), requires_grad= False) \n",
    "        else: \n",
    "            class_weights = np.array(class_weights) \n",
    "            assert nclass == class_weights.shape[0] \n",
    "            self.class_weights = nn.Parameter(torch.tensor(class_weights, dtype= torch.float32), requires_grad= False) \n",
    "    \n",
    "    def prob_forward(self, pred, target, mask= None): \n",
    "        size = pred.size() \n",
    "        N, nclass = size[0], size[1] \n",
    "\n",
    "        # N x C x H x W \n",
    "        pred_one_hot = pred.view(N, nclass, -1) \n",
    "        target = target.view(N, 1, -1) \n",
    "        target_one_hot = to_one_hot(target.type(torch.long), nclass).type(torch.float32)\n",
    "\n",
    "        # N x C x H x W \n",
    "        inter = pred_one_hot * target_one_hot\n",
    "        union = pred_one_hot + target_one_hot\n",
    "\n",
    "        if mask is not None: \n",
    "            mask = mask.view(N, 1, -1) \n",
    "            inter = (inter.view(N, nclass, -1) * mask).sum(2) \n",
    "            union = (union.view(N, nclass, -1) * mask).sum(2) \n",
    "        else: \n",
    "            inter = inter.view(N, nclass, -1).sum(2) \n",
    "            union = union.view(N, nclass, -1).sum(2)\n",
    "        \n",
    "        dice = ( 2*inter + self.smooth ) / (union + self.smooth) \n",
    "        return 1 - dice.mean()\n",
    "\n",
    "    def forward(self, logits,target, mask = None): \n",
    "        size = logits.size() \n",
    "        N, nclass = size[0], size[1] \n",
    "\n",
    "        logits = logits.view(N, nclass, -1) \n",
    "        target = target.view(N, 1, -1) \n",
    "\n",
    "        pred,nclass = get_probability(logits) \n",
    "\n",
    "        pred_one_hot = pred \n",
    "        target_one_hot = to_one_hot(target.type(torch.long), nclass).type(torch.float32) \n",
    "\n",
    "        inter = pred_one_hot * target_one_hot\n",
    "        union = pred_one_hot + target_one_hot\n",
    "\n",
    "        if mask is not None: \n",
    "            mask = mask.view(N, 1, -1) \n",
    "            inter = (inter.view(N, nclass, -1) * mask).sum(2)\n",
    "            union = (union.view(N, nclass, -1) * mask ).sum(2) \n",
    "        else: \n",
    "            inter = inter.view(N, nclass, -1).sum(2) \n",
    "            union = union.view(N, nclass, -1).sum(2)\n",
    "        \n",
    "        dice = ( 2 * inter + self.smooth ) / (union + self.smooth)\n",
    "        return 1 - dice.mean() \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNet3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Sequential\n",
    "from torch.nn import Conv3d, ConvTranspose3d, BatchNorm3d, MaxPool3d, AvgPool3d, AvgPool1d, Dropout3d\n",
    "from torch.nn import ReLU, Sigmoid\n",
    "import torch\n",
    "import pdb\n",
    "\n",
    "class UNet(Module):\n",
    "    def __init__(self, in_dimension=1, out_dimension=2, ft_channels=[64, 256, 256, 512, 1024], residual='conv'):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder downsamplers\n",
    "        self.pool1 = MaxPool3d((2, 2, 2))\n",
    "        self.pool2 = MaxPool3d((2, 2, 2))\n",
    "        self.pool3 = MaxPool3d((2, 2, 2))\n",
    "        self.pool4 = MaxPool3d((2, 2, 2))\n",
    "        \n",
    "        # Encoder convolutions\n",
    "        self.conv_block1 = Conv3D_Block(in_dimension, ft_channels[0], residual=residual)\n",
    "        self.conv_block2 = Conv3D_Block(ft_channels[0], ft_channels[1], residual=residual)\n",
    "        self.conv_block3 = Conv3D_Block(ft_channels[1], ft_channels[2], residual=residual)\n",
    "        self.conv_block4 = Conv3D_Block(ft_channels[2], ft_channels[3], residual=residual)\n",
    "        self.conv_block5 = Conv3D_Block(ft_channels[3], ft_channels[4], residual=residual)\n",
    "        \n",
    "        # Decoderr convolutions\n",
    "        self.decoder_conv_block4 = Conv3D_Block(2 * ft_channels[3], ft_channels[3], residual=residual)\n",
    "        self.decoder_conv_block3 = Conv3D_Block(2 * ft_channels[2], ft_channels[2], residual=residual)\n",
    "        self.decoder_conv_block2 = Conv3D_Block(2 * ft_channels[1], ft_channels[1], residual=residual)\n",
    "        self.decoder_conv_block1 = Conv3D_Block(2 * ft_channels[0], ft_channels[0], residual=residual)\n",
    "        \n",
    "        # Decoder upsamplers\n",
    "        self.deconv_block4 = Deconv3D_Block(ft_channels[4], ft_channels[3])\n",
    "        self.deconv_block3 = Deconv3D_Block(ft_channels[3], ft_channels[2])\n",
    "        self.deconv_block2 = Deconv3D_Block(ft_channels[2], ft_channels[1])\n",
    "        self.deconv_block1 = Deconv3D_Block(ft_channels[1], ft_channels[0])\n",
    "        \n",
    "        # Final 1*1 Convolutions segmentation map\n",
    "        self.one_conv = Conv3d(ft_channels[0], out_dimension, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        \n",
    "        # Activation function\n",
    "        self.sigmoid = Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Encoder part\n",
    "        x1 = self.conv_block1(x)\n",
    "        x_low1 = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.conv_block2(x_low1)\n",
    "        x_low2 = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.conv_block3(x_low2)\n",
    "        x_low3 = self.pool3(x3)\n",
    "        \n",
    "        x4 = self.conv_block4(x_low3)\n",
    "        x_low4 = self.pool4(x4)\n",
    "        \n",
    "        base = self.conv_block5(x_low4)\n",
    "        \n",
    "        # Decoder part\n",
    "        d4 = torch.cat([self.deconv_block4(base), x4], dim=1)\n",
    "        d_high4 = self.decoder_conv_block4(d4)\n",
    "        \n",
    "        d3 = torch.cat([self.deconv_block3(d_high4), x3], dim=1)\n",
    "        d_high3 = self.decoder_conv_block3(d3)\n",
    "        d_high3 = Dropout3d(p=0.05)(d_high3)\n",
    "        \n",
    "        d2 = torch.cat([self.deconv_block2(d_high3), x2], dim=1)\n",
    "        d_high2 = self.decoder_conv_block2(d2)\n",
    "        d_high2 = Dropout3d(p=0.05)(d_high2)\n",
    "        \n",
    "        d1 = torch.cat([self.deconv_block1(d_high2), x1], dim=1)\n",
    "        d_high1 = self.decoder_conv_block1(d1)\n",
    "        \n",
    "        seg = self.one_conv(d_high1)\n",
    "        \n",
    "        return seg\n",
    "\n",
    "        \n",
    "class Conv3D_Block(Module):\n",
    "    def __init__(self, in_features, out_features, kernel=3, stride=1, padding=1, residual=None):\n",
    "        super(Conv3D_Block, self).__init__()\n",
    "        \n",
    "        self.conv1 = Sequential(\n",
    "            Conv3d(in_features, out_features, kernel_size=kernel, stride=stride, padding=padding, bias=True),\n",
    "            BatchNorm3d(out_features),\n",
    "            ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv2 = Sequential(\n",
    "            Conv3d(out_features, out_features, kernel_size=kernel, stride=stride, padding=padding, bias=True),\n",
    "            BatchNorm3d(out_features),\n",
    "            ReLU()\n",
    "        )\n",
    "        \n",
    "        self.residual = residual\n",
    "        \n",
    "        if self.residual is not None:\n",
    "            self.residual_upsampler = Conv3d(in_features, out_features, kernel_size=1, bias=False)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        res = x\n",
    "        \n",
    "        if not self.residual:\n",
    "            return self.conv2(self.conv1(x))\n",
    "        else:\n",
    "            return self.conv2(self.conv1(x)) + self.residual_upsampler(res)\n",
    "        \n",
    "class Deconv3D_Block(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, kernel=3, stride=2, padding=1):\n",
    "        super(Deconv3D_Block, self).__init__()\n",
    "        \n",
    "        self.deconv = Sequential(\n",
    "            ConvTranspose3d(in_features, out_features, kernel_size=(kernel, kernel, kernel),\n",
    "                            stride=(stride, stride, stride), padding=(padding, padding, padding), output_padding=1, bias=True),\n",
    "            ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.deconv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_rampup(current, rampup_length):\n",
    "    if rampup_length == 0: \n",
    "        return 1.0 \n",
    "    else:\n",
    "        current = np.clip(current, 0, rampup_length)\n",
    "        phase = 1 - (current / rampup_length)\n",
    "        return float(np.exp(-5 * phase * phase))\n",
    "    \n",
    "# Mean-Teacher compomnent \n",
    "def get_current_consistency_weight(epoch, args): \n",
    "    return 5 * args.consistency + sigmoid_rampup(epoch, args.consistency_rampup)\n",
    "\n",
    "def update_model_ema(model, ema_model, alpha): \n",
    "    model_state = model.state_dict() \n",
    "    model_ema_state = ema_model.state_dict()\n",
    "\n",
    "\n",
    "    new_dict = {} \n",
    "\n",
    "    for key in model_state:\n",
    "        new_dict[key] = alpha * model_ema_state[key] + (1 - alpha) * model_state[key]\n",
    "\n",
    "    ema_model.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cut_mask(out, thres=0.5, nms=0):\n",
    "    probs = F.softmax(out, 1)\n",
    "    masks = (probs >= thres).type(torch.int64)\n",
    "    masks = masks[:, 1, :, :].contiguous()\n",
    "    if nms == 1:\n",
    "        masks = LargestCC_pancreas(masks)\n",
    "    return masks\n",
    "\n",
    "def LargestCC_pancreas(segmentation):\n",
    "    N = segmentation.shape[0]\n",
    "    batch_list = []\n",
    "    for n in range(N):\n",
    "        n_prob = segmentation[n].detach().cpu().numpy()\n",
    "        labels = label(n_prob)\n",
    "        if labels.max() != 0:\n",
    "            largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1\n",
    "        else:\n",
    "            largestCC = n_prob\n",
    "        batch_list.append(largestCC)\n",
    "    \n",
    "    return torch.Tensor(batch_list).cuda()\n",
    "\n",
    "def save_net_opt(net, optimizer, path):\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'opt': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(state, str(path))\n",
    "\n",
    "def load_net_opt(net, optimizer, path):\n",
    "    state = torch.load(str(path))\n",
    "    net.load_state_dict(state['net'])\n",
    "    optimizer.load_state_dict(state['opt'])\n",
    "\n",
    "def load_net(net, path):\n",
    "    state = torch.load(str(path))\n",
    "    net.load_state_dict(state['net'])\n",
    "\n",
    "def get_current_consistency_weight(epoch):\n",
    "    return args.consistency * sigmoid_rampup(epoch, args.consistency_rampup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = args.root_path \n",
    "pre_max_iterations = args.pre_max_iterations\n",
    "self_max_iterations = args.self_max_iteration \n",
    "base_lr = args.base_lr \n",
    "CE = nn.CrossEntropyLoss(reduction= 'none')\n",
    "\n",
    "\n",
    "if args.deterministic:\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
