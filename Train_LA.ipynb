{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import h5py \n",
    "import os \n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "import sys\n",
    "from skimage.measure import label\n",
    "from scipy.ndimage import zoom, rotate \n",
    "from medpy import metric \n",
    "from tqdm import tqdm \n",
    "import logging \n",
    "import os \n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import Sampler \n",
    "import torchvision.transforms as transforms \n",
    "import torch.optim as optim \n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params \n",
    "class params: \n",
    "    def __init__(self):\n",
    "        self.root_path = '/kaggle/input/la-dataset/LA'\n",
    "        self.exp = 'BCP' \n",
    "        self.model = 'VNet'\n",
    "        self.pre_max_iterations = 10\n",
    "        self.self_max_iteration = 100 \n",
    "        self.max_samples = 80 \n",
    "        self.labeled_bs = 4\n",
    "        self.batch_size = 8 \n",
    "        self.base_lr = 0.01 \n",
    "        self.deterministic = 1 \n",
    "        self.labelnum = 8 \n",
    "        self.consistency = 1.0 \n",
    "        self.consistency_rampup = 40.0 \n",
    "        self.magnitude = 10.0 \n",
    "        self.seed = 10\n",
    "        self.gpu = '0'\n",
    "    \n",
    "        # Setting of BCP \n",
    "        self.u_weight = 0.5 \n",
    "        self.mask_ratio = 2/3 \n",
    "        self.patch_size = [112, 112, 80]\n",
    "\n",
    "        # Setting of mixup \n",
    "        self.u_alpha = 2.0 \n",
    "        self.loss_weight = 0.5\n",
    "\n",
    "\n",
    "args = params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset): \n",
    "    def __init__(self, root_path, split= 'train', transform= None, num= None): \n",
    "        \"\"\"\n",
    "        Use to load name of file.list \n",
    "\n",
    "        \"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.split = split \n",
    "        self.transform = transform\n",
    "        self.sample_list = []\n",
    "        \n",
    "        # Select dataset ( ACDC or LA )\n",
    "        if self.root_path == 'ACDC': \n",
    "            train_list = os.path.join(self.root_path, 'train_slices.list')\n",
    "            val_list = os.path.join(self.root_path, 'val.list')\n",
    "            self.train_files = os.path.join(self.root_path, 'data/slices')\n",
    "            self.valid_files = os.path.join(self.root_path, 'data')\n",
    "        elif self.root_path == 'LA': \n",
    "            train_list = os.path.join(self.root_path, 'train.list')\n",
    "            self.train_files = os.path.join(self.root_path, '2018LA_Seg_Training Set' )\n",
    "        \n",
    "        modes = ['train', 'val'] if self.root_path == 'ACDC' else ['train']\n",
    "        if self.split in modes: \n",
    "            # Create sample_list \n",
    "            if self.split == 'train': \n",
    "                with open(train_list, 'r') as file: \n",
    "                    self.sample_list = file.readlines() \n",
    "                \n",
    "                self.sample_list = [item.replace('\\n', '') for item in self.sample_list]\n",
    "            elif self.split == 'val': \n",
    "                with open(val_list, 'r') as file: \n",
    "                    self.sample_list = file.readlines() \n",
    "                \n",
    "                self.sample_list = [item.replace('\\n','') for item in self.sample_list]\n",
    "        else: \n",
    "            raise ValueError(f'Mode: {self.split} is not supported')\n",
    "        \n",
    "        # Use number of dataset only \n",
    "        if isinstance(num, int): \n",
    "            self.sample_list = self.sample_list[:num]\n",
    "        \n",
    "        # print(f'Total slices: {len(self.sample_list)}')\n",
    "        \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        case = self.sample_list[index]\n",
    "        # Prepare the file_path\n",
    "        if self.root_path == 'ACDC':\n",
    "            if self.split == 'train':  \n",
    "                file_path = os.path.join(self.train_files, f'{case}.h5')\n",
    "            else: \n",
    "                file_path = os.path.join(self.valid_files, f'{case}.h5')\n",
    "        elif self.root_path == 'LA': \n",
    "            file_path = os.path.join(self.train_files, f'{case}/mri_norm2.h5')\n",
    "\n",
    "        # Extract data\n",
    "        h5f = h5py.File(file_path, 'r')\n",
    "        image = h5f['image'][:]\n",
    "        label = h5f['label'][:]\n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        # Apply transform \n",
    "        if self.split == 'train' and self.transform is not None: \n",
    "            sample = self.transform(sample)\n",
    "        sample['case'] = case \n",
    "        return sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rot_flip(image, label): \n",
    "    # rotate \n",
    "    k = np.random.randint(0, 4)\n",
    "    image = np.rot90(image, k) \n",
    "    label = np.rot90(label, k) \n",
    "    # flip \n",
    "    axis = np.random.randint(0, 2)\n",
    "    image = np.flip(image, axis) \n",
    "    label = np.flip(label, axis) \n",
    "    return image, label \n",
    "\n",
    "\n",
    "class RandomRotFlip: \n",
    "    def __call__(self, sample): \n",
    "        image, label = sample['image'], sample['label']\n",
    "        image, label = random_rot_flip(image, label)\n",
    "        sample = {'image': image, 'label': label}\n",
    "        return sample\n",
    "    \n",
    "class RandomCrop: \n",
    "    def __init__(self, output_size, padding_margin = 3 ): \n",
    "        self.output_size = output_size \n",
    "        self.padding_margin = padding_margin \n",
    "\n",
    "    def __call__(self, sample): \n",
    "        image, label = sample['image'] , sample['label'] # (112 ,80, 80)\n",
    "\n",
    "        # Padding if need \n",
    "        if label.shape[0] < self.output_size[0] or label.shape[1] < self.output_size[1] or label.shape[2] < self.output_size[2]: \n",
    "            w_pad = max((self.output_size[0] - label.shape[0]) // 2 + self.padding_margin, 0) \n",
    "            h_pad = max((self.output_size[1] - label.shape[1]) // 2 + self.padding_margin, 0)\n",
    "            d_pad = max((self.output_size[2] - label.shape[2]) // 2 + self.padding_margin, 0) \n",
    "            print(f'({w_pad, h_pad, d_pad})')\n",
    "            image = np.pad(image, [(w_pad, w_pad), (h_pad, h_pad), (d_pad, d_pad)], mode= 'constant', constant_values= 0)\n",
    "            label = np.pad(label, [(w_pad, w_pad), (h_pad, h_pad), (d_pad, d_pad)], mode= 'constant', constant_values= 0) \n",
    "\n",
    "        # Random crop \n",
    "        w, h, d = image.shape \n",
    "        w1 = np.random.randint(0, w - self.output_size[0])\n",
    "        h1 = np.random.randint(0, h - self.output_size[1])\n",
    "        d1 = np.random.randint(0, d - self.output_size[2])\n",
    "\n",
    "        image = image[w1 : w1 + self.output_size[0], h1 : h1 + self.output_size[1], d1 : d1 + self.output_size[2]]\n",
    "        label = label[w1 : w1 + self.output_size[0], h1 : h1 + self.output_size[1], d1 : d1 + self.output_size[2]]\n",
    "\n",
    "        sample = {'image': image, 'label': label} \n",
    "        return sample\n",
    "\n",
    "class ToTensor: \n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'].copy(), sample['label'].copy() # how the fuck i know ?\n",
    "        image = np.reshape(image, (1, image.shape[0], image.shape[1], image.shape[2])).astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            'image': torch.from_numpy(image).to(torch.float32), \n",
    "            'label': torch.from_numpy(label).to(torch.int64)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_once(indices): \n",
    "    \"\"\"\n",
    "    Permutate the iterable once \n",
    "    (permutate the labeled_idxs once)\n",
    "    \"\"\"\n",
    "    return np.random.permutation(indices) \n",
    "\n",
    "def iterate_externally(indices): \n",
    "    \"\"\"\n",
    "    Create an infinite iterator that repeatedly permutes the indices.\n",
    "    ( permutate the unlabeled_idxs to make different)\n",
    "    \"\"\"\n",
    "    def infinite_shuffles(): \n",
    "        while True: \n",
    "            yield np.random.permutation(indices)\n",
    "            \n",
    "    return itertools.chain.from_iterable(infinite_shuffles())\n",
    "\n",
    "def grouper(iterable, n): \n",
    "    args = [iter(iterable)] * n \n",
    "    return zip(*args)\n",
    "\n",
    "class TwoStreamBatchSampler(Sampler): \n",
    "    def __init__(self, primary_indicies, secondary_indicies, batchsize, secondary_batchsize): \n",
    "        self.primary_indicies = primary_indicies\n",
    "        self.secondary_indicies = secondary_indicies\n",
    "        self.primary_batchsize = batchsize - secondary_batchsize\n",
    "        self.secondary_batchsize = secondary_batchsize\n",
    "\n",
    "        assert len(self.primary_indicies) >= self.primary_batchsize > 0 \n",
    "        assert len(self.secondary_indicies) >= self.secondary_batchsize > 0 \n",
    "\n",
    "    def __iter__(self):\n",
    "        primary_iter = iterate_once(self.primary_indicies)\n",
    "        secondary_iter = iterate_externally(self.secondary_indicies)\n",
    "\n",
    "        return (\n",
    "            primary_batch + secondary_batch\n",
    "            for (primary_batch, secondary_batch) \n",
    "            in zip(grouper(primary_iter, self.primary_batchsize),\n",
    "                   grouper(secondary_iter, self.secondary_batchsize))\n",
    "        )\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.primary_indicies) // self.primary_batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(tensor, num_classes): \n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - tensor (torch.Tensor): NxDxHxW \n",
    "        - num_classes (int): num classes \n",
    "    \"\"\"\n",
    "    assert tensor.max().item() < num_classes \n",
    "    assert tensor.min().item() >= 0 \n",
    "\n",
    "    size = list(tensor.size())\n",
    "    assert size[1] == 1 \n",
    "    size[1] = num_classes \n",
    "    one_hot = torch.zeros(*size)\n",
    "\n",
    "    if tensor.is_cuda: \n",
    "        one_hot = one_hot.to(tensor.device) \n",
    "    \n",
    "    one_hot = one_hot.scatter_(1, tensor, 1) # create onehot form ? \n",
    "    return one_hot \n",
    "\n",
    "def get_probability(logits): \n",
    "    \"\"\"\n",
    "    Params: \n",
    "        - logits (torch.Tensor): prediction of model\n",
    "    \"\"\"\n",
    "    size = logits.size() \n",
    "    if size[1] > 1: \n",
    "        pred = F.softmax(logits, dim= 1) \n",
    "        nclass = size[1] \n",
    "    else: # is it necessary ?  \n",
    "        pred = F.sigmoid(logits) \n",
    "        pred = torch.cat([1 - pred, pred], dim= 1) \n",
    "    \n",
    "    return pred, nclass \n",
    "\n",
    "\n",
    "class DiceLoss3D(nn.Module):\n",
    "    def __init__(self, num_classes, class_weights= None, smooth= 1e-5): \n",
    "        super(DiceLoss3D, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.smooth = smooth \n",
    "        if class_weights is None: \n",
    "            self.class_weights = nn.Parameter(torch.ones(1, num_classes).type(torch.float32), requires_grad= False)\n",
    "        else: \n",
    "            class_weights = np.array(class_weights) \n",
    "            self.class_weights = nn.Parameter(torch.tensor(class_weights, dtype= torch.float32), requires_grad= False) \n",
    "    \n",
    "    #TODO: I seee it dont nesscessary  \n",
    "    def prob_forward(self): \n",
    "        pass \n",
    "        \n",
    "    def forward(self, logits, target, mask= None): \n",
    "        size = logits.size()\n",
    "        N, nclass = size[0], size[1]\n",
    "\n",
    "        logits = logits.view(N, 1, -1) \n",
    "        target = target.view(N, 1, -1) \n",
    "\n",
    "        pred_one_hot, nclass = get_probability(logits) \n",
    "        target_one_hot = to_one_hot(target.type(torch.long), nclass).type(torch.float32) \n",
    "\n",
    "        inter = pred_one_hot * target_one_hot\n",
    "        union = pred_one_hot + target_one_hot \n",
    "\n",
    "        if mask is not None: \n",
    "            mask = mask.view(N, 1, -1) \n",
    "            inter = (inter.view(N, 1, -1) * mask).sum(2) \n",
    "            union = (union.view(N, 1, -1) * mask).sum(2) \n",
    "        else: \n",
    "            inter = inter.view(N, 1, -1).sum(2) \n",
    "            union = union.view(N, 1, -1).sum(2) \n",
    "        \n",
    "        dice = (2 * inter + self.smooth) / (union + self.smooth) \n",
    "        return 1 - dice\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICE = DiceLoss3D(num_classes= 2) \n",
    "CE = nn.CrossEntropyLoss(reduction= 'none')\n",
    "def mix_loss(output, img_l, patch_l, mask, l_weight = 1.0, u_weight = 0.5, unlab = False): \n",
    "    img_l, patch_l = img_l.type(torch.int64), patch_l.type(torch.int64)\n",
    "    image_weight, patch_weight = l_weight, u_weight \n",
    "    if unlab: \n",
    "        image_weight, patch_weight = u_weight, l_weight \n",
    "    \n",
    "    patch_mask = 1 - mask \n",
    "    loss_dice = DICE(output, img_l, mask) * image_weight\n",
    "    loss_dice += DICE(output, patch_l, patch_mask) * patch_weight\n",
    "\n",
    "    loss_ce = image_weight * (CE(output, img_l) * mask).sum() / (mask.sum() + 1e-16)\n",
    "    loss_ce += patch_weight * (CE(output, patch_l) * patch_mask).sum() / (patch_mask.sum() + 1e-16)\n",
    "    loss = (loss_dice + loss_ce) / 2 \n",
    "    return loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_case(model, image, stride_xy, stride_z, patch_size , num_classes= 1): \n",
    "    w, h, d = image.shape \n",
    "    pad_w, pad_h, pad_d = max(0, patch_size[0] - w), max(0, patch_size[1] -h), max(0, patch_size[2] - d)\n",
    "    pad = [(pad_w // 2, pad_w - pad_w //2), \n",
    "           (pad_h // 2, pad_h - pad_h //2), \n",
    "           (pad_d // 2, pad_d - pad_d //2)]\n",
    "    \n",
    "    if any(p > 0 for p in [pad_w, pad_h, pad_d]): \n",
    "        image = np.pad(image, pad, mode= 'constant', constant_values= 0) \n",
    "    \n",
    "    ww, hh, dd = image.shape \n",
    "\n",
    "    # Calculate the number of patches along each dimension \n",
    "    sx = math.ceil((ww - patch_size[0]) / stride_xy) + 1 \n",
    "    sy = math.ceil((hh - patch_size[1]) / stride_xy) + 1 \n",
    "    sz = math.ceil((dd - patch_size[2]) / stride_z) + 1 \n",
    "\n",
    "    # Initialize score map and count map \n",
    "    score_map = np.zeros((num_classes, ww, hh, dd), dtype= np.float32) \n",
    "    cnt = np.zeros((ww, hh, dd), dtype= np.float32) \n",
    "\n",
    "    # Perform sliding-window inference \n",
    "    for x in range(sx): \n",
    "        xs = min(stride_xy *x , ww - patch_size[0])\n",
    "        for y in range(y): \n",
    "            ys = min(stride_xy * y, hh - patch_size[1])\n",
    "            for z in range(sz): \n",
    "                zs = min(stride_z * z, dd - patch_size[2]) \n",
    "                \n",
    "\n",
    "def var_all_case_LA(model, num_classes, patch_size = (112, 112, 80), stride_xy= 18, stride_z = 4): \n",
    "    pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
